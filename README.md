# LINEAR REGRESSION FROM SCRATCH

<p align="center">
  <img src="https://img.shields.io/github/last-commit/DaRkSouL36/LinearRegression?style=for-the-badge&color=blueviolet" />
  <img src="https://img.shields.io/github/repo-size/DaRkSouL36/LinearRegression?style=for-the-badge&color=blue" />
  <img src="https://img.shields.io/github/languages/count/DaRkSouL36/LinearRegression?style=for-the-badge&color=green" />
  <img src="https://img.shields.io/github/languages/top/DaRkSouL36/LinearRegression?style=for-the-badge&color=orange" />
  <img src="https://img.shields.io/github/license/DaRkSouL36/LinearRegression?style=for-the-badge&color=red" />
  <br><br>

<h1 align="center">PURE MATHEMATICS. NO BLACK BOXES.</h1>

> <p align="center"><b>MASTERING MACHINE LEARNING FUNDAMENTALS BY BUILDING THEM FROM THE GROUND UP.</b></p>
> <p align="center"><b>BUILD IT. UNDERSTAND IT. VERIFY IT.</b></p>

<div style="text-align: justify;">
A COMPREHENSIVE IMPLEMENTATION OF LINEAR REGRESSION BUILT ENTIRELY FROM SCRATCH USING ONLY NUMPY. THIS PROJECT ELIMINATES THE ABSTRACTION OF LIBRARIES LIKE SCIKIT-LEARN TO REVEAL THE UNDERLYING CALCULUS AND LINEAR ALGEBRA DRIVING MACHINE LEARNING. IT FEATURES CUSTOM IMPLEMENTATIONS OF GRADIENT DESCENT VARIANTS, THE NORMAL EQUATION, AND DYNAMIC VISUALIZATIONS.
</div>

---

## üìñ TABLE OF CONTENTS

- [LINEAR REGRESSION FROM SCRATCH](#linear-regression-from-scratch)
  - [üìñ TABLE OF CONTENTS](#-table-of-contents)
  - [üìò PROJECT OVERVIEW](#-project-overview)
  - [üí¨ FEATURES](#-features)
  - [üìÅ NOTEBOOKS BREAKDOWN](#-notebooks-breakdown)
    - [`ROOT DIRECTORY`](#root-directory)
  - [üß† MATHEMATICAL CONCEPTS IMPLEMENTED](#-mathematical-concepts-implemented)
    - [1. THE HYPOTHESIS](#1-the-hypothesis)
    - [2. COST FUNCTION (MSE)](#2-cost-function-mse)
    - [3. GRADIENT DESCENT UPDATE RULE](#3-gradient-descent-update-rule)
    - [4. NORMAL EQUATION](#4-normal-equation)
  - [üìä METHODS IMPLEMENTED](#-methods-implemented)
    - [üîÅ OPTIMIZATION-BASED METHODS](#-optimization-based-methods)
    - [üìê ANALYTICAL METHOD](#-analytical-method)
    - [üìè EVALUATION METRICS (FROM SCRATCH)](#-evaluation-metrics-from-scratch)
  - [üìä VISUALIZATION HIGHLIGHTS](#-visualization-highlights)
  - [‚öôÔ∏è HOW IT WORKS](#Ô∏è-how-it-works)
  - [üèÅ USAGE](#-usage)
  - [üìà RESULTS AND CONCLUSION](#-results-and-conclusion)
    - [‚úî KEY OBSERVATIONS](#-key-observations)
  - [üöÄ FUTURE WORK](#-future-work)
  - [üìÑ LICENSE](#-license)
  - [üì¶ INSTALLATION INSTRUCTIONS](#-installation-instructions)

---

## üìò PROJECT OVERVIEW

THIS REPOSITORY SERVES AS A DEEP DIVE INTO THE MECHANICS OF REGRESSION ANALYSIS. INSTEAD OF RELYING ON `model.fit()`, THIS PROJECT MANUALLY IMPLEMENTS THE OPTIMIZATION LOOPS, COST FUNCTION CALCULATIONS, AND PARAMETER UPDATES. IT IS DESIGNED TO DEMONSTRATE THE MATHEMATICAL FIDELITY OF CUSTOM IMPLEMENTATIONS BY BENCHMARKING THEM AGAINST INDUSTRY-STANDARD LIBRARIES.

---

## üí¨ FEATURES

- **ZERO ML LIBRARIES:** BUILT STRICTLY WITH `NUMPY`, `PANDAS`, AND `MATPLOTLIB`.
- **MULTIPLE OPTIMIZERS:** INCLUDES BATCH, STOCHASTIC (SGD), AND MINI-BATCH GRADIENT DESCENT.
- **ANALYTICAL SOLUTION:** IMPLEMENTATION OF THE **NORMAL EQUATION** FOR EXACT SOLUTIONS.
- **ADVANCED ANALYSIS:** LEARNING RATE DECAY, CONVERGENCE MONITORING, ETC.
- **BENCHMARKING:** COMPARATIVE ANALYSIS AGAINST `SCIKIT-LEARN` TO VERIFY ACCURACY.

---

## üìÅ NOTEBOOKS BREAKDOWN

### `ROOT DIRECTORY`

- **LinearRegression_GradientDescent.ipynb** ‚Äì THE CORE IMPLEMENTATION COVERING BATCH, STOCHASTIC, AND MINI-BATCH GRADIENT DESCENT ALGORITHMS ALONG WITH LEARNING RATE DECAY ANALYSIS AND EXTENSIVE VISUALIZATIONS.
- **LinearRegression_NormalEquation.ipynb** ‚Äì THE ANALYTICAL APPROACH USING MATRIX OPERATIONS (LINEAR ALGEBRA) TO SOLVE FOR PARAMETERS DIRECTLY WITHOUT ITERATION.
- **DATA/** ‚Äì CONTAINS THE INPUT DATASETS `linearX.csv` AND `linearY.csv`.

---

## üß† MATHEMATICAL CONCEPTS IMPLEMENTED

### 1. THE HYPOTHESIS
$$h_\theta(x) = \theta_0 + \theta_1 x$$

### 2. COST FUNCTION (MSE)
$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$$

### 3. GRADIENT DESCENT UPDATE RULE
$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$$

### 4. NORMAL EQUATION
$$\theta = (X^T X)^{-1} X^T y$$

---

## üìä METHODS IMPLEMENTED

### üîÅ OPTIMIZATION-BASED METHODS
- BATCH GRADIENT DESCENT  
- STOCHASTIC GRADIENT DESCENT (SGD)  
- MINI-BATCH GRADIENT DESCENT  
- LEARNING RATE DECAY FOR SGD  

### üìê ANALYTICAL METHOD
- NORMAL EQUATION  
- PSEUDO-INVERSE FOR NUMERICAL STABILITY  

### üìè EVALUATION METRICS (FROM SCRATCH)
- MEAN ABSOLUTE ERROR (MAE)  
- MEAN SQUARED ERROR (MSE)  
- ROOT MEAN SQUARED ERROR (RMSE)  
- R¬≤ SCORE (COEFFICIENT OF DETERMINATION)  

---

## üìä VISUALIZATION HIGHLIGHTS

- **CONVERGENCE GRAPHS:** PLOTS COST FUNCTION VS. ITERATIONS TO VISUALIZE OPTIMIZATION SPEED.
- **SGD NOISE ANALYSIS:** COMPARES THE STABLE PATH OF BATCH GD AGAINST THE NOISY PATH OF STOCHASTIC GD.
- **LEARNING RATE DECAY:** DEMONSTRATES HOW DECAYING LR STABILIZES SGD OSCILLATIONS NEAR THE MINIMUM.
- **REGRESSION FITS:** OVERLAYS THE LEARNED REGRESSION LINE ON RAW DATA POINTS.

---

## ‚öôÔ∏è HOW IT WORKS

1.  **DATA LOADING:** READS RAW CSV DATA AND PERFORMS Z-SCORE NORMALIZATION.
2.  **MODEL INITIALIZATION:** SETS HYPERPARAMETERS (LEARNING RATE, EPOCHS, BATCH SIZE).
3.  **OPTIMIZATION:**
    * **GRADIENT DESCENT:** ITERATIVELY UPDATES WEIGHTS TO MINIMIZE ERROR.
    * **NORMAL EQUATION:** SOLVES FOR WEIGHTS DIRECTLY USING LINEAR ALGEBRA.
4.  **EVALUATION:** CALCULATES $R^2$ SCORE, MAE, MSE, AND RMSE MANUALLY.
5.  **VISUALIZATION:** PLOTS COST HISTORY, REGRESSION FIT, AND ANIMATIONS.
   
---

## üèÅ USAGE

1.  **SELECT THE NOTEBOOK:** CHOOSE BETWEEN ITERATIVE (GD) OR ANALYTICAL (NORMAL EQ) METHODS.
2.  **RUN THE CELLS:** EXECUTE PREPROCESSING AND TRAINING STEPS.
3.  **EXPERIMENT:** MODIFY `learning_rate` OR `batch_size` IN THE CLASS INSTANTIATION TO SEE DIFFERENT BEHAVIORS.

---

## üìà RESULTS AND CONCLUSION

<div style="text-align: justify;">
THE CUSTOM IMPLEMENTATIONS ACHIEVED NEAR-IDENTICAL RESULTS TO SCIKIT-LEARN, VALIDATING THE MATHEMATICAL CORRECTNESS OF THE ALGORITHMS.

- **BATCH GD** MATCHED SCIKIT-LEARN PREDICTIONS WITH A DIFFERENCE OF APPROXIMATELY **0.00048**.
- **NORMAL EQUATION** DERIVED THE EXACT ANALYTICAL OPTIMUM.
- **SGD WITH DECAY** SUCCESSFULLY CONVERGED DEEPER THAN FIXED LR SGD.

THE PROJECT DEMONSTRATES THAT BUILDING ALGORITHMS FROM SCRATCH IS THE BEST WAY TO DEMYSTIFY MACHINE LEARNING BLACK BOXES.
</div>

### ‚úî KEY OBSERVATIONS
- REGRESSION LINES FROM SCRATCH AND SCIKIT-LEARN OVERLAP CLOSELY  
- NORMAL EQUATION MATCHES SCIKIT-LEARN EXACTLY  
- GRADIENT DESCENT CONVERGES TO THE SAME SOLUTION SPACE  
- LEARNING RATE STRONGLY INFLUENCES CONVERGENCE SPEED AND STABILITY
  
---

## üöÄ FUTURE WORK

- ADD SUPPORT FOR **MULTIPLE LINEAR REGRESSION** (MULTIVARIATE).
- IMPLEMENT **POLYNOMIAL REGRESSION** FROM SCRATCH.
- ADD **L1 (LASSO) AND L2 (RIDGE) REGULARIZATION**.
- CREATE A WEB INTERFACE FOR UPLOADING DATASETS AND VISUALIZING RESULTS.

---

## üìÑ LICENSE

THIS PROJECT IS LICENSED UNDER THE [MIT LICENSE](LICENSE).

YOU ARE FREE TO USE, MODIFY, AND DISTRIBUTE THIS PROJECT WITH ATTRIBUTION.

---

## üì¶ INSTALLATION INSTRUCTIONS

> 1. **CLONE THE REPOSITORY**

---

`git clone https://github.com/DaRkSouL36/LinearRegression`

`cd "LinearRegression"`

---

> 2. **CREATE AND ACTIVATE VIRTUAL ENVIRONMENT**

---

`python -m venv VENV`

- LINUX/MAC
  
   `source VENV/bin/activate`

- ON WINDOWS
  
   `VENV\Scripts\activate`

---

> 3. **INSTALL REQUIRED DEPENDENCIES** 

---

`pip install -r REQUIREMENTS.txt`

---

<p>
  <a href="#linear-regression-from-scratch">
    <img src="https://img.icons8.com/fluency/48/up.png" width="20px"/>
    <strong>RETURN</strong>
  </a>
</p>