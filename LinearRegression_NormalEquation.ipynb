{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04cd961a",
   "metadata": {},
   "source": [
    "# NORMAL EQUATION: THE CLOSED-FORM SOLUTION\n",
    "\n",
    "UNLIKE GRADIENT DESCENT, WHICH IS AN ITERATIVE OPTIMIZATION ALGORITHM THAT STEPS TOWARDS THE MINIMUM, THE **NORMAL EQUATION** CALCULATES THE OPTIMAL PARAMETERS ANALYTICALLY IN A SINGLE STEP.\n",
    "\n",
    "### THE MATHEMATICAL FORMULA\n",
    "THE OPTIMAL WEIGHTS $\\theta$ (THETA) THAT MINIMIZE THE COST FUNCTION $J(\\theta)$ CAN BE CALCULATED DIRECTLY USING LINEAR ALGEBRA:\n",
    "\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "### KEY DIFFERENCES FROM GRADIENT DESCENT\n",
    "| FEATURE | GRADIENT DESCENT | NORMAL EQUATION |\n",
    "| :--- | :--- | :--- |\n",
    "| **TYPE** | ITERATIVE (APPROXIMATE) | ANALYTICAL (EXACT) |\n",
    "| **LEARNING RATE** | REQUIRED ($\\alpha$) | NOT REQUIRED |\n",
    "| **ITERATIONS** | MANY REQUIRED | NONE (1 STEP) |\n",
    "| **COMPLEXITY** | $O(kn^2)$ (GOOD FOR LARGE DATASETS) | $O(n^3)$ (SLOW FOR LARGE FEATURE SETS) |\n",
    "| **FEATURE SCALING**| MANDATORY FOR SPEED | NOT STRICTLY NECESSARY |\n",
    "\n",
    "### IMPLEMENTATION STRATEGY\n",
    "1. **ADD BIAS TERM:** APPEND A COLUMN OF 1s TO THE FEATURE MATRIX $X$ TO HANDLE THE INTERCEPT ($\\theta_0$).\n",
    "2. **COMPUTE:** USE `NUMPY` FOR MATRIX MULTIPLICATION AND INVERSION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60ed1b",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bace0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING NECESSARY LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcbb7c6",
   "metadata": {},
   "source": [
    "## LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a5304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING DATA FROM CSV FILES\n",
    "df_x = pd.read_csv('linearX.csv')\n",
    "df_y = pd.read_csv('linearY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4eedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERTING TO NUMPY ARRAYS\n",
    "\n",
    "X_raw = df_x.values.flatten()\n",
    "y = df_y.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22566e",
   "metadata": {},
   "source": [
    "## DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee7c07",
   "metadata": {},
   "source": [
    "> `NOTE:` \n",
    ">\n",
    "> NORMAL EQUATION DOESN'T STRICTLY NEED THIS, BUT WE KEEP IT FOR FAIR COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fff76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING: Z-SCORE NORMALIZATION (STANDARDIZATION)\n",
    "# FORMULA: Z = (X - MEAN) / STD_DEV\n",
    "# THIS HELPS GRADIENT DESCENT CONVERGE FASTER\n",
    "\n",
    "# CALCULATING MEAN AND STANDARD DEVIATION \n",
    "\n",
    "mean_x = np.sum(X_raw) / len(X_raw)\n",
    "std_x = np.sqrt(np.sum((X_raw - mean_x)**2) / len(X_raw))\n",
    "\n",
    "# APPLYING NORMALIZATION\n",
    "X = (X_raw - mean_x) / std_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb0539",
   "metadata": {},
   "source": [
    "## LINEAR REGRESSION â€” NORMAL EQUATION EXPLANATION\n",
    "\n",
    "---\n",
    "\n",
    "## ORIGINAL LINEAR MODEL (SCALAR FORM)\n",
    "\n",
    "IN SIMPLE LINEAR REGRESSION, THE MODEL FOR A SINGLE INSTANCE IS DEFINED AS:\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "WHERE:\n",
    "* $\\theta_0$ : **INTERCEPT (BIAS)**\n",
    "* $\\theta_1$ : **SLOPE (WEIGHT)**\n",
    "* $x$ : **INPUT FEATURE**\n",
    "* $y$ : **PREDICTED OUTPUT**\n",
    "\n",
    "---\n",
    "\n",
    "## PROBLEM WITH THE SCALAR FORM\n",
    "\n",
    "> THE SCALAR FORM IS COMPUTATIONALLY INEFFICIENT BECAUSE:\n",
    "> * IT IS DIFFICULT TO SCALE TO **MULTIPLE FEATURES** ($x_1, x_2, ..., x_n$)\n",
    "> * IT PREVENTS THE USE OF HIGH-PERFORMANCE **MATRIX ALGEBRA**\n",
    "> * IT CANNOT BE SOLVED DIRECTLY VIA THE **NORMAL EQUATION**\n",
    "\n",
    "---\n",
    "\n",
    "## THE DESIGN MATRIX ($X_b$)\n",
    "\n",
    "WE INTRODUCE A **BIAS FEATURE** ($x_0 = 1$) BY PREPENDING A COLUMN OF ONES TO OUR FEATURE MATRIX. THIS TRANSFORMS OUR DATA INTO THE **DESIGN MATRIX**:\n",
    "\n",
    "$$X_b = \\begin{bmatrix} 1 & x^{(1)} \\\\ 1 & x^{(2)} \\\\ \\vdots & \\vdots \\\\ 1 & x^{(m)} \\end{bmatrix}$$\n",
    "\n",
    "**NUMPY IMPLEMENTATION:**\n",
    "`X_b = np.c_[np.ones((m, 1)), X]`\n",
    "\n",
    "---\n",
    "\n",
    "## PARAMETER VECTOR ($\\theta$)\n",
    "\n",
    "BY INCORPORATING THE BIAS INTO THE MATRIX, WE CAN STACK ALL PARAMETERS INTO A SINGLE COLUMN VECTOR:\n",
    "\n",
    "$$\\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix}$$\n",
    "\n",
    "---\n",
    "\n",
    "## VECTORIZED LINEAR MODEL\n",
    "\n",
    "NOW, THE ENTIRE SET OF PREDICTIONS FOR ALL $m$ EXAMPLES IS REDUCED TO A SINGLE MATRIX MULTIPLICATION:\n",
    "\n",
    "$$\\hat{y} = X_b \\cdot \\theta$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## THE NORMAL EQUATION (CLOSED-FORM SOLUTION)\n",
    "\n",
    "THE **NORMAL EQUATION** IS THE ANALYTICAL SOLUTION THAT FINDS THE VALUE OF $\\theta$ THAT MINIMIZES THE COST FUNCTION WITHOUT REQUIRING GRADIENT DESCENT:\n",
    "\n",
    "$$\\theta = (X_b^T X_b)^{-1} X_b^T y$$\n",
    "\n",
    "### MATRIX COMPONENT BREAKDOWN\n",
    "\n",
    "| COMPONENT | MATHEMATICAL OPERATION |\n",
    "| :--- | :--- |\n",
    "| $X_b^T$ | **TRANSPOSE** OF THE DESIGN MATRIX |\n",
    "| $(X_b^T X_b)^{-1}$ | **MATRIX INVERSE** OF THE COVARIANCE-LIKE MATRIX |\n",
    "| $y$ | **TARGET VECTOR** (ACTUAL LABELS) |\n",
    "\n",
    "---\n",
    "\n",
    "## KEY ADVANTAGES OF THIS APPROACH\n",
    "\n",
    "* **UNIFICATION:** THE INTERCEPT $\\theta_0$ IS TREATED EXACTLY LIKE OTHER WEIGHTS.\n",
    "* **SPEED:** VECTORIZATION LEVERAGES **BLAS** LIBRARIES FOR PARALLEL COMPUTATION.\n",
    "* **PRECISION:** PROVIDES THE EXACT MATHEMATICAL MINIMUM IN A SINGLE CALCULATION.\n",
    "\n",
    "---\n",
    "\n",
    "## FINAL SUMMARY\n",
    "\n",
    "> $$\\hat{y} = X_b \\theta \\implies \\theta = (X_b^T X_b)^{-1} X_b^T y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28c30b",
   "metadata": {},
   "source": [
    "## NORMAL EQUATION IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0e71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ADDING BIAS TERM (COLUMN OF ONES: X0 = 1) TO X ---\n",
    "# X MATRIX SHAPE BECOMES (m, 2)\n",
    "# X_b = [1, x_1\n",
    "#        1, x_2\n",
    "#        ...\n",
    "#        1, x_m]\n",
    "\n",
    "# THE FORMULA REQUIRES X TO BE A DESIGN MATRIX [1, x1, x2...]\n",
    "\n",
    "m = len(y)\n",
    "\n",
    "# ADD x0 = 1 TO EACH INSTANCE\n",
    "X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "# CALCULATING THETA USING NORMAL EQUATION\n",
    "# FORMULA: theta = inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# COMPUTE X TRANSPOSE DOT X\n",
    "X_T_X = X_b.T.dot(X_b)\n",
    "\n",
    "# COMPUTE INVERSE OF (X_T_X)\n",
    "# WE USE pseudo-inverse (pinv) FOR STABILITY IF MATRIX IS SINGULAR\n",
    "inv_X_T_X = np.linalg.pinv(X_T_X)\n",
    "\n",
    "# COMPUTE X TRANSPOSE DOT Y\n",
    "X_T_y = X_b.T.dot(y)\n",
    "\n",
    "# FINAL CALCULATION\n",
    "theta = inv_X_T_X.dot(X_T_y)\n",
    "\n",
    "# EXTRACTING PARAMETERS\n",
    "b_norm = theta[0]\n",
    "w_norm = theta[1]\n",
    "\n",
    "# PREDICTION FUNCTION FOR NORMAL EQUATION\n",
    "def predict_normal(X_input):\n",
    "    return w_norm * X_input + b_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8f80f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- NORMAL EQUATION RESULTS ---\n",
      "INTERCEPT (BIAS) --> 0.996634\n",
      "SLOPE (WEIGHT) -->   0.001358\n"
     ]
    }
   ],
   "source": [
    "print(\"--- NORMAL EQUATION RESULTS ---\")\n",
    "print(f\"INTERCEPT (BIAS) --> {b_norm:.6f}\")\n",
    "print(f\"SLOPE (WEIGHT) -->   {w_norm:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7119b3a",
   "metadata": {},
   "source": [
    "## COMPARISON: GD vs NORMAL EQ vs SKLEARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae80a8",
   "metadata": {},
   "source": [
    "LINEAR REGRESSION GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5cf05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \n",
    "    # INITIALIZING HYPERPARAMETERS\n",
    "    def __init__(self, learning_rate = 0.5, iterations = 10):\n",
    "        self.lr = learning_rate\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        self.w = 0.0\n",
    "        self.b = 0.0\n",
    "    \n",
    "    # TRAINING USING GRADIENT DESCENT\n",
    "    def train(self, X, y):\n",
    "        m = len(y)\n",
    "        \n",
    "        # GRADIENT DESCENT LOOP\n",
    "        for _ in range(self.iterations):\n",
    "            \n",
    "            pred = self.w * X + self.b\n",
    "                \n",
    "            # UPDATING PARAMETERS\n",
    "            self.w -= self.lr * (1 / m) * np.sum((pred - y) * X)\n",
    "            self.b -= self.lr * (1 / m) * np.sum(pred - y)\n",
    "            \n",
    "        return self.w, self.b\n",
    "    \n",
    "# INITIALIZING MODEL\n",
    "model = LinearRegression(learning_rate = 0.5, iterations = 10)\n",
    "\n",
    "# TRAINING USING BATCH GRADIENT DESCENT\n",
    "w_gd, b_gd = model.train(X, y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eced76",
   "metadata": {},
   "source": [
    "SCIKIT LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba0bd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT SCIKIT LEARN\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# TRAINING SKLEARN MODEL\n",
    "model_sk = LinearRegression()\n",
    "model_sk.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "w_sk = model_sk.coef_[0]\n",
    "b_sk = model_sk.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1132c9",
   "metadata": {},
   "source": [
    "COMPARISON DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "041ffb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL PARAMETER COMPARISON ---\n",
      "             METHOD    WEIGHT      BIAS\n",
      "0   NORMAL EQUATION  0.001358  0.996634\n",
      "1  GRADIENT DESCENT  0.001357  0.995661\n",
      "2      SCIKIT-LEARN  0.001358  0.996634\n"
     ]
    }
   ],
   "source": [
    "comparison_data = {\n",
    "    'METHOD' : ['NORMAL EQUATION', 'GRADIENT DESCENT', 'SCIKIT-LEARN'],\n",
    "    'WEIGHT' : [w_norm, w_gd, w_sk],\n",
    "    'BIAS' : [b_norm, b_gd, b_sk]\n",
    "}\n",
    "\n",
    "df_comp = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n--- FINAL PARAMETER COMPARISON ---\")\n",
    "print(df_comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
